# model configuration
model: 8B-Instruct
load_in_nbits: 4
custom_checkpoint_path: null
# training configuration
training:
  efficient_finetuning_method: "lora" # set load_in_nbits=4 for qlora
  lora_apply_layers:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - embed_tokens
  lora_dropout: 0.1
  lora_r: 8
  lora_alpha: 32
  reft_component: null
  reft_low_rank_dim: null
# optimization config
optimization:
  lr: 0.0003
  betas:
    - 0.9
    - 0.999
  eps: 1.0E-06
  weight_decay: 0.001
  batch_size: 4
  gradient_accumulation_steps: 1
epochs: 3
# inference will be configured by frontend